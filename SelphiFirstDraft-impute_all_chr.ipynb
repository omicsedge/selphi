{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zarr\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import BidiBurrowsWheelerLibrary\n",
    "from data_utils import get_sample_index, remove_all_samples\n",
    "\n",
    "from variations import run_hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "chrom = \"chr1\"\n",
    "SI_data_folder = f\"./new_data/SI_data/{chrom}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f7(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/beagle_data/imputed_samples.txt\",mode=\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    samples = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = f7(samples)\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "with open(f\"{SI_data_folder}/chip_id_list.txt\", \"rb\") as fp:   # Unpickling\n",
    "    chip_id_list = pickle.load(fp)\n",
    "with open(f\"{SI_data_folder}/full_id_list_full.txt\", \"rb\") as fp:   # Unpickling\n",
    "    full_id_list = pickle.load(fp)\n",
    "with open(f\"{SI_data_folder}/original_indicies_full.txt\", \"rb\") as fp:   # Unpickling\n",
    "    original_indicies = pickle.load(fp)\n",
    "chr_length = len(full_id_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GENETIC MAP\n",
    "import os\n",
    "# PUT genetic map path here that beagle uses\n",
    "genetic_map_path = f\"/home/ubuntu/resource_files/genetic_map_plink/plink.{chrom}.GRCh38.map\"\n",
    "\n",
    "def get_cm(recomb_maps, pos):\n",
    "    # TODO: Change default pop\n",
    "    for i in list(recomb_maps[\"default\"].keys()):\n",
    "        if pos <= i:\n",
    "            return recomb_maps[\"default\"][i]\n",
    "    return list(recomb_maps[\"default\"].keys())[-1]\n",
    "\n",
    "map_pops = [\"default\",]\n",
    "recomb_maps = {x:z for (x,z) in [[x, {k:v for (k,v) in [[row[3],row[2]] for _, row in pd.read_csv(genetic_map_path, sep=\" \", comment=\"#\").iterrows()]}] for x in map_pops]}\n",
    "chip_positions_dedup = [int(x.split('-')[1]) for x in chip_id_list]\n",
    "num_obs = len(chip_positions_dedup)\n",
    "distances_cm = [get_cm(recomb_maps, x) for x in chip_positions_dedup]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_map = pd.read_csv(genetic_map_path, sep=\" \", comment=\"#\",header=None,usecols=[0,2,3])\n",
    "genetic_map.columns = ['chr','cM','pos']\n",
    "genetic_map.set_index('pos',inplace=True)\n",
    "genetic_map = genetic_map.join(pd.DataFrame(chip_positions_dedup).set_index(0),how='outer')\n",
    "genetic_map['cM'] = genetic_map['cM'].interpolate('index').fillna(method='bfill')\n",
    "genetic_map['chr'] = genetic_map.chr.fillna(20.0).astype(int)\n",
    "genetic_map = genetic_map.reset_index().rename({'index':'pos'},axis=1)\n",
    "genetic_map = genetic_map[genetic_map['pos'].isin(chip_positions_dedup)].reset_index(drop=True)\n",
    "distances_cm = genetic_map.cM\n",
    "distances_cm = list(distances_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std(avg_length, a=25):\n",
    "    # convert average length into number on x axis\n",
    "    rmin_ = 10000\n",
    "    rmax_ = 1\n",
    "    tmin_ = 0\n",
    "    tmax_ = 1\n",
    "    avg_length = ((avg_length - rmin_)/(rmax_ - rmin_)) * (tmax_ - tmin_) + tmin_\n",
    "    \n",
    "    std_not_normed = a*avg_length**(a-1.)\n",
    "    \n",
    "    rmin_ = 0\n",
    "    rmax_ = a*1**(a-1.)\n",
    "    tmin_ = 0.2\n",
    "    tmax_ = 3\n",
    "    return ((std_not_normed - rmin_)/(rmax_ - rmin_)) * (tmax_ - tmin_) + tmin_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_panel_full_array_full_packed = zarr.load(\"./new_data/SI_data/chr1/reference_panel.30x.hg38_chr1_noinfo_full_nppacked.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HG02330\n",
      "Concatenated shape:  (49977, 5822)\n",
      "Build BI BJ: main imputation DS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49977/49977 [01:12<00:00, 693.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporary fix: for chip_variants with no matches, all are taken as matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49977/49977 [00:14<00:00, 3480.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial composite panel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49977/49977 [00:16<00:00, 3005.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Haploid frequencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49977/49977 [00:03<00:00, 14922.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating haploid count thresholds for each chip variant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49977/49977 [00:07<00:00, 6554.40it/s]\n",
      "100%|██████████| 49977/49977 [00:28<00:00, 1776.10it/s]\n",
      "100%|██████████| 49977/49977 [00:34<00:00, 1430.50it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from data_utils import remove_all_samples\n",
    "CHUNK_SIZE = num_obs\n",
    "HAPS = [0,1]\n",
    "\n",
    "for sample in samples[12:]:\n",
    "    print(sample)\n",
    "    full_res_ = {}\n",
    "    sample_index = get_sample_index(sample, samples_txt_path=f\"./new_data/SI_data/samples.txt\")\n",
    "    target_full_array = np.zeros((chr_length,2))\n",
    "    target_full_array[:,0] = np.unpackbits(ref_panel_full_array_full_packed[:,sample_index[0]])[:chr_length]\n",
    "    target_full_array[:,1] = np.unpackbits(ref_panel_full_array_full_packed[:,sample_index[1]])[:chr_length]\n",
    "\n",
    "    ref_panel_full_array = remove_all_samples(ref_panel_full_array_full_packed)\n",
    "#     ref_panel_full_array = remove_all_samples(ref_panel_full_array_full)\n",
    "#     ref_panel_full_array = zarr.load(\"./new_data/SI_data/chr1/reference_panel.30x.hg38_chr1_noinfo_full_nppacked.zip\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ref_panel_chip_array = ref_panel_full_array[original_indicies,:]\n",
    "    ref_panel_chip_array = zarr.load(f\"{SI_data_folder}/reference_panel.30x.hg38_{chrom}_noinfo_chip_variants_292samples_removed.zip\")\n",
    "    \n",
    "    target_chip_array = target_full_array[original_indicies,:]\n",
    "    combined_ref_panel_chip = np.concatenate([ref_panel_chip_array,target_chip_array],axis=1)\n",
    "    print(\"Concatenated shape: \", combined_ref_panel_chip.shape)\n",
    "\n",
    "    for hap in HAPS:\n",
    "\n",
    "        bidi_pbwt = BidiBurrowsWheelerLibrary(combined_ref_panel_chip.T.astype(np.int8), ref_panel_chip_array.shape[1]+hap)\n",
    "        ppa_matrix = bidi_pbwt.getForward_Ppa()\n",
    "        div_matrix = bidi_pbwt.getForward_Div()\n",
    "        rev_ppa_matrix = bidi_pbwt.getBackward_Ppa()\n",
    "        rev_div_matrix = bidi_pbwt.getBackward_Div()\n",
    "\n",
    "        forward_pbwt_matches, forward_pbwt_hap_indices = bidi_pbwt.getForward_matches_indices()\n",
    "        backward_pbwt_matches, backward_pbwt_hap_indices = bidi_pbwt.getBackward_matches_indices()\n",
    "\n",
    "        num_chip_vars = ppa_matrix.shape[1]\n",
    "        num_hid = ref_panel_chip_array.shape[1]\n",
    "\n",
    "        BI = np.zeros((num_hid,num_chip_vars))\n",
    "        BJ = np.zeros((num_hid,num_chip_vars))\n",
    "        \n",
    "        print(\"Build BI BJ: main imputation DS\")\n",
    "        for chip_var in trange(0,num_chip_vars):\n",
    "            forward_pbwt_matches_=forward_pbwt_matches[:,chip_var]\n",
    "            forward_pbwt_index=ppa_matrix[:,chip_var]\n",
    "            backward_pbwt_matches_=backward_pbwt_matches[:,chip_var]\n",
    "            backward_pbwt_index=np.flip(rev_ppa_matrix, axis=1)[:,chip_var]\n",
    "            BI[:,chip_var] = forward_pbwt_matches_[forward_pbwt_index.argsort()][:num_hid]\n",
    "            BJ[:,chip_var] = backward_pbwt_matches_[backward_pbwt_index.argsort()][:num_hid]\n",
    "\n",
    "            \n",
    "        print(\"temporary fix: for chip_variants with no matches, all are taken as matches\")\n",
    "        for chip_var in trange(0,num_chip_vars):\n",
    "            \n",
    "            x = np.unique(BI[:,chip_var])\n",
    "            if len(x) == 1 and x[0] == 0:\n",
    "                BI[:,chip_var] = 1\n",
    "                BJ[:,chip_var] = 1\n",
    "\n",
    "        matches = BI + BJ -1\n",
    "        fl = 13\n",
    "        \n",
    "        print(\"Creating initial composite panel\")\n",
    "        composite_ = np.zeros(matches.shape)\n",
    "        best_matches = {}\n",
    "        for chip_index in trange(0, matches.shape[1]):\n",
    "            best_matches[chip_index] = list(np.argsort(matches[:,chip_index])[::-1][:fl])\n",
    "            for hap_index in best_matches[chip_index]:\n",
    "                composite_[hap_index ,chip_index:int(chip_index + BJ[hap_index, chip_index])] = 1\n",
    "                composite_[hap_index ,int(chip_index - BI[hap_index, chip_index] + 1):chip_index+1] = 1\n",
    "\n",
    "#         WOW = ((BI == 1).astype(int) * (BJ > 10).astype(int) * BJ)\n",
    "#         for i in trange(0, matches.shape[1]):\n",
    "#             x = np.unique(WOW[:,i], return_counts=True)\n",
    "#             if len(x[0]) < 25:\n",
    "#                 continue\n",
    "\n",
    "#             perc = np.percentile(x[1][1:],99)\n",
    "#             index_ = np.where(x[1][1:] > perc)[0]\n",
    "#             repeats = x[0][1:][index_]\n",
    "#             hapsi = []\n",
    "#             [hapsi.extend(np.where(BJ[:,i] == x)[0]) for x in range(0,len(repeats))]\n",
    "#             for hapso in hapsi:\n",
    "#                 composite_[hapso,i: i + int(BJ[hapso, i])] = 0\n",
    "\n",
    "        comp_matches_hybrid = (composite_ * matches).astype(int)\n",
    "        \n",
    "        print(\"Calculating Haploid frequencies\")\n",
    "        hap_freq = {}\n",
    "        for i in trange(0,matches.shape[1]):\n",
    "            chunk_index = int(i//CHUNK_SIZE)\n",
    "            hap_freq.setdefault(chunk_index, {})\n",
    "            \n",
    "            indexoo = np.flatnonzero(comp_matches_hybrid[:,i])\n",
    "            for indexo in indexoo:\n",
    "                hap_freq[chunk_index][indexo] = hap_freq[chunk_index].get(indexo, 0) + 1\n",
    "\n",
    "        haps_freqs_array_norm_dict = {}\n",
    "        for chunk_index in list(hap_freq.keys()):\n",
    "            \n",
    "            haps_freqs_array = np.zeros(matches.shape[0])\n",
    "            for key, item in hap_freq[chunk_index].items():\n",
    "                haps_freqs_array[key] = item\n",
    "\n",
    "            rmin = min(haps_freqs_array)\n",
    "            rmax = max(haps_freqs_array)\n",
    "            tmin = 0.1\n",
    "            tmax = 1\n",
    "            haps_freqs_array_norm = (((haps_freqs_array - rmin)/(rmax - rmin)) * (tmax - tmin) + tmin)\n",
    "            haps_freqs_array_norm_dict[chunk_index] = haps_freqs_array_norm.copy()\n",
    "            \n",
    "        # STD_THRESH\n",
    "        averages = []\n",
    "        hybrid = (composite_ * matches).astype(int)\n",
    "        for i in range(0,matches.shape[1]):\n",
    "            averages.append(np.average(hybrid[np.flatnonzero(hybrid[:,i]),i]))\n",
    "    \n",
    "        std_thresh = list(get_std(np.array(averages)))\n",
    "\n",
    "\n",
    "        std_ = []\n",
    "        lengthos = []\n",
    "        final_thresh = []\n",
    "        nc_thresh = []\n",
    "        print(\"Calculating haploid count thresholds for each chip variant\")\n",
    "        for i in trange(0,matches.shape[1]):\n",
    "            chunk_index = int(i//CHUNK_SIZE)\n",
    "            indexoo = np.flatnonzero(comp_matches_hybrid[:,i] * haps_freqs_array_norm_dict[chunk_index])\n",
    "            X = comp_matches_hybrid[indexoo,i] * haps_freqs_array_norm_dict[chunk_index][indexoo]\n",
    "            lengthos.append(X[np.argsort(X)[::-1]])\n",
    "            std_.append(np.std(X[np.argsort(X)[::-1]]))\n",
    "            final_thresh.append(max(lengthos[i]) - std_thresh[i]*std_[i])\n",
    "            nc_thresh.append(\n",
    "                len(np.flatnonzero(lengthos[i] >= final_thresh[i]))\n",
    "            )\n",
    "\n",
    "        composite_ = np.zeros(matches.shape)\n",
    "        best_matches = {}\n",
    "        for chip_index in trange(0, matches.shape[1]):\n",
    "            chunk_index = int(chip_index//CHUNK_SIZE)\n",
    "            xooi = matches[:,chip_index] * haps_freqs_array_norm_dict[chunk_index]\n",
    "            best_matches[chip_index] = list(np.argsort(xooi)[::-1][:nc_thresh[chip_index]])\n",
    "            for hap_index in best_matches[chip_index]:\n",
    "                composite_[hap_index ,chip_index:int(chip_index + BJ[hap_index, chip_index])] = 1\n",
    "                composite_[hap_index ,int(chip_index - BI[hap_index, chip_index] + 1):chip_index+1] = 1\n",
    "    \n",
    "        \n",
    "        ordered_matches_test__ = {}\n",
    "        comp_to_plot = np.zeros(composite_.shape)\n",
    "        for i in trange(BJ.shape[1]):\n",
    "            xooi = matches[:,i]\n",
    "            sorting_key = list(xooi.argsort()[::-1])\n",
    "        #     sorting_key = list(np.argsort(haps_freqs_array_norm)[::-1])\n",
    "\n",
    "            uniqes = list(np.where(composite_[:,i] == 1)[0])\n",
    "            ordered_matches_test__[i] = sorted(uniqes,key=sorting_key.index)\n",
    "            comp_to_plot[ordered_matches_test__[i],i] = 1\n",
    "            if len(ordered_matches_test__[i]) == 0:\n",
    "                ordered_matches_test__[i] = list(np.where(matches[:,i] != 0)[0])\n",
    "        \n",
    "        length_matches = {}\n",
    "        length_matches_normalized = {}\n",
    "        for i in range(0,matches.shape[1]):\n",
    "            for j in ordered_matches_test__[i]:\n",
    "                length_matches.setdefault(i, []).append(int(matches[j, i]))\n",
    "            rmin = min(length_matches[i])\n",
    "            rmax = max(length_matches[i])\n",
    "            if rmax == rmin:\n",
    "                rmin = rmin-1\n",
    "            tmin = 0\n",
    "            tmax = 0\n",
    "            length_matches_normalized[i] = list(np.round((((np.array(length_matches[i]) - rmin)/(rmax - rmin)) * (tmax - tmin) + tmin),5))\n",
    "        \n",
    "        print(\"Start Imputing\")\n",
    "        resultoo_fb = run_hmm(\n",
    "            original_indicies,\n",
    "            ref_panel_full_array,\n",
    "            num_obs,\n",
    "            ordered_matches_test__,\n",
    "            distances_cm,\n",
    "            BI, BJ,\n",
    "            # THIS VARIABLE IS USELESS, YOU CAN REFACTOR AND REMOVE\n",
    "            length_matches_normalized,\n",
    "            chr_length,\n",
    "            num_hid=matches.shape[0],\n",
    "            \n",
    "#             imputed_chr_length=\n",
    "        )\n",
    "#         resultoo_fb = run_hmm_variable_N(\n",
    "#             original_indicies,\n",
    "#             ref_panel_full_array,\n",
    "#             num_obs,\n",
    "#             ordered_matches_test__,\n",
    "#             distances_cm,\n",
    "#             nc_thresh,\n",
    "#             length_matches_normalized,\n",
    "#         )\n",
    "        START = original_indicies[0]\n",
    "        END = original_indicies[-1]\n",
    "        print(\"DONE\")\n",
    "\n",
    "        new_x = target_full_array[:,hap]\n",
    "        y = resultoo_fb\n",
    "        print(int(y.shape - np.sum(new_x == y)))\n",
    "\n",
    "        full_res_.setdefault(sample, []).append(resultoo_fb.copy())\n",
    "\n",
    "        if hap == 1:\n",
    "            print(\"saving results\")\n",
    "            with open(f'./method_first_draft/saved_dictionary_{str(sample)}_new_method_{chrom}.pkl', 'wb') as f:\n",
    "                pickle.dump(full_res_, f)\n",
    "            arr__1 = full_res_[sample][0]\n",
    "            arr__2 = full_res_[sample][1]\n",
    "            final_arr = arr__1 + arr__2\n",
    "\n",
    "            y_1 = target_full_array[:,0]\n",
    "            y_2 = target_full_array[:,1]\n",
    "            final_y = y_1 + y_2\n",
    "            print(\"Full results\")\n",
    "            print(int(final_arr.shape - np.sum(final_y == final_arr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beagle_res(file_name):\n",
    "    beagle_array = pd.read_csv(file_name, sep=\"\\t\", header=None, comment=\"#\")\n",
    "    beagle_array.drop([0,1,3,4,5,6,7,8],axis=1,inplace=True)\n",
    "    concat_beagle = pd.concat([\n",
    "        beagle_array.applymap(lambda x: str(x).split(\":\")[0].replace(\"/\",\"|\").split(\"|\")[0]),\n",
    "        beagle_array.applymap(lambda x: str(x).split(\":\")[0].replace(\"/\",\"|\").split(\"|\")[-1]),\n",
    "    ],axis=1)\n",
    "    concat_beagle.drop([2],axis=1,inplace=True)\n",
    "    concat_beagle.columns = [0,1]\n",
    "    return concat_beagle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"HG02429\"\n",
    "results = get_beagle_res(f\"./data/beagle_data/292_BEAGLE_5.4_mapYES_neYES_validated_chr1_{sample}.vcf.gz\")\n",
    "beagle_0 = results[0].to_numpy().astype(int)\n",
    "beagle_1 = results[1].to_numpy().astype(int)\n",
    "final_arr_beagle = beagle_0 + beagle_1\n",
    "\n",
    "sample_index = get_sample_index(sample, samples_txt_path=f\"./new_data/SI_data/samples.txt\")\n",
    "target_full_array = np.zeros((chr_length,2))\n",
    "target_full_array[:,0] = np.unpackbits(ref_panel_full_array_full_packed[:,sample_index[0]])[:chr_length]\n",
    "target_full_array[:,1] = np.unpackbits(ref_panel_full_array_full_packed[:,sample_index[1]])[:chr_length]\n",
    "\n",
    "y_1 = target_full_array[:,0]\n",
    "y_2 = target_full_array[:,1]\n",
    "final_y = y_1 + y_2\n",
    "        \n",
    "print(int(final_arr_beagle.shape - np.sum(final_y == final_arr_beagle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS PLOT DOESN\"T WORK YET NOW\n",
    "\n",
    "# PLOT results\n",
    "\n",
    "# del plot_results\n",
    "import utils_no_mod\n",
    "# importlib.reload(utils_no_mod)\n",
    "from utils_no_mod import plot_results\n",
    "folder_dict = {\n",
    "    \"beagle\":\"/home/ubuntu/selphi/data/validation_data/beagle_validated/\",\n",
    "#     \"selphi HMM\":\"./\",\n",
    "    \"selphi HMM\":\"/home/ubuntu/selphi/method_first_draft/\",\n",
    "}\n",
    "\n",
    "# OVERWRITE SAMPLES\n",
    "# samples = [\n",
    "#     \"HG02470\",\n",
    "# ]\n",
    "\n",
    "plot_results(\n",
    "    samples,\n",
    "    folder_dict,\n",
    "    ref_panel_full_array_full,\n",
    "    target_full_array,\n",
    "    original_indicies,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
